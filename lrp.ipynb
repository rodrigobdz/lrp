{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LRP\n",
    "\n",
    "Implementation of Layer-wise Relevance Propagation (LRP) in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myutils\n",
    "img = myutils.load_normalized_img('castle.jpg')\n",
    "X = myutils.img_to_tensor(img)\n",
    "\n",
    "import torchvision\n",
    "model = torchvision.models.vgg16(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "import utils\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Parameter modifier\n",
    "# max(0, *)\n",
    "positive_contributions = lambda p: p.clamp(min=0)\n",
    "# min(0, *)\n",
    "negative_contributions = lambda p: p.clamp(max=0)\n",
    "\n",
    "# Build an equivalent forward pass where part of it is detached\n",
    "class ZeroEpsilonGammaConv(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, conv, epsilon=0, gamma=0):\n",
    "    torch.nn.Module.__init__(self)\n",
    "    self.epsilon = epsilon\n",
    "    self.conv = conv\n",
    "    # p stands for processed\n",
    "    self.pconv = copy.deepcopy(conv)\n",
    "    self.pconv.weight = torch.nn.Parameter(conv.weight + gamma * positive_contributions(conv.weight))\n",
    "    self.pconv.bias = torch.nn.Parameter(conv.bias + gamma * positive_contributions(conv.bias))\n",
    "\n",
    "  def forward(self, X):\n",
    "    z = self.conv.forward(X)\n",
    "    zp = self.epsilon + self.pconv.forward(X)\n",
    "    # The additive term 1e-9 enforces 0/0 = 0 (numerical stability)\n",
    "    return zp * (z / (zp + 1e-9)).detach()\n",
    "    # return zp * (z / zp).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from myutils import ILSVRC2012_BatchNormalize\n",
    "batch_size = 1\n",
    "shape = (batch_size, 3, 224, 224)\n",
    "norm_fn = ILSVRC2012_BatchNormalize()\n",
    "low = norm_fn(torch.zeros(*shape))\n",
    "high = norm_fn(torch.ones(*shape))\n",
    "\n",
    "class ZBConv(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, conv, low, high):\n",
    "    torch.nn.Module.__init__(self)\n",
    "    self.conv = conv\n",
    "    self.low = low\n",
    "    self.high = high\n",
    "    self.low.requires_grad = True\n",
    "    self.high.requires_grad = True\n",
    "    \n",
    "    self.low_conv = copy.deepcopy(conv)\n",
    "    self.high_conv = copy.deepcopy(conv)\n",
    "\n",
    "    self.low_conv.weight = torch.nn.Parameter(positive_contributions(conv.weight))\n",
    "    self.low_conv.bias = torch.nn.Parameter(positive_contributions(conv.bias))\n",
    "\n",
    "    self.high_conv.weight = torch.nn.Parameter(negative_contributions(conv.weight))\n",
    "    self.high_conv.bias = torch.nn.Parameter(negative_contributions(conv.bias))\n",
    "\n",
    "  def forward(self, X):\n",
    "    z_conv = self.conv.forward(X)\n",
    "    z_low = self.low_conv.forward(self.low)\n",
    "    z_high = self.high_conv.forward(self.high)\n",
    "\n",
    "    z = z_conv - z_low - z_high\n",
    "\n",
    "    return z * (z_conv / (z + 1e-9)).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRP():\n",
    "  def __init__(self, model) -> None:\n",
    "      self.model = model\n",
    "      self.model.eval()\n",
    "\n",
    "  def convert_single_layer(self, i, layer, classifier=False):\n",
    "    if i == 0 and not classifier:\n",
    "      # First layer is the pixel layer\n",
    "      # return layer\n",
    "      return ZBConv(layer, low=low, high=high)\n",
    "      \n",
    "    # # Source: lrp-tutorial\n",
    "    # if isinstance(layers[l],torch.nn.MaxPool2d):\n",
    "    #   layers[l] = torch.nn.AvgPool2d(2)\n",
    "    #\n",
    "    # if isinstance(layer, torch.nn.MaxPool2d):\n",
    "    #   layer = ZeroEpsilonGammaConv(layer, gamma)\n",
    "\n",
    "    # Case: bottom layers, apply LRP-Gamma with gamma = 0.25\n",
    "    if i <= 16: gamma = 0.25; epsilon = 0\n",
    "\n",
    "    # Case: middle layers, apply LRP-Epsilon with epsilon = 0.25. Alt. 1e-6\n",
    "    if 17 <= i <= 30: gamma = 0; epsilon = 0.25\n",
    "\n",
    "    # Case: top layers, apply LRP-0\n",
    "    if i >= 31: gamma = 0; epsilon = 0\n",
    "\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "      return ZeroEpsilonGammaConv(layer, epsilon=epsilon, gamma=gamma)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "  def convert_layers(self):\n",
    "    for i, layer in enumerate(self.model.features):\n",
    "      self.model.features[i] = self.convert_single_layer(i, layer)\n",
    "\n",
    "    self.model.avgpool = self.convert_single_layer(31, layer)\n",
    "    \n",
    "    for i, layer in enumerate(self.model.classifier):\n",
    "      self.model.classifier[i] = self.convert_single_layer(32+i, layer, classifier=True)\n",
    "\n",
    "  def relevance(self, X):\n",
    "    # Apply Gradient x Input\n",
    "    # Prepare to compute input gradient\n",
    "\n",
    "    # Reset gradient\n",
    "    self.model.zero_grad()\n",
    "    X.requires_grad = True\n",
    "\n",
    "    # Compute explanation\n",
    "    # Stores value of gradient in X.grad\n",
    "    # [0].max() retrieves the maximum activation/relevance in the first layer\n",
    "    # = 483 for castle\n",
    "    self.model.forward(X)[0].max().backward()\n",
    "\n",
    "    # Retrieve gradients from first layer\n",
    "    first_layer = self.model.features[0]\n",
    "    l = first_layer.low\n",
    "    h = first_layer.high\n",
    "    \n",
    "    # Calculate gradients\n",
    "    c1, c2, c3 = X.grad, l.grad, h.grad\n",
    "\n",
    "    # Calculate relevance\n",
    "    self.R = X*c1 + l*c2 + h*c3\n",
    "    return self.R\n",
    "\n",
    "  def visualize(self):\n",
    "    utils.heatmap(self.R[0].sum(dim=0).detach().numpy(), 4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x4608 and 25088x4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5g/63b_p4412n15h04497bsytd80000gp/T/ipykernel_66668/5378499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlrp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelevance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/5g/63b_p4412n15h04497bsytd80000gp/T/ipykernel_66668/1835230906.py\u001b[0m in \u001b[0;36mrelevance\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# [0].max() retrieves the maximum activation/relevance in the first layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# = 483 for castle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Retrieve gradients from first layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/unterlagen/bildung/uni/master/masterarbeit/code/lrp/venv/lib/python3.9/site-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/unterlagen/bildung/uni/master/masterarbeit/code/lrp/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/unterlagen/bildung/uni/master/masterarbeit/code/lrp/venv/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/unterlagen/bildung/uni/master/masterarbeit/code/lrp/venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/unterlagen/bildung/uni/master/masterarbeit/code/lrp/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/unterlagen/bildung/uni/master/masterarbeit/code/lrp/venv/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x4608 and 25088x4096)"
     ]
    }
   ],
   "source": [
    "lrp = LRP(model)\n",
    "lrp.convert_layers()\n",
    "lrp.relevance(X)\n",
    "lrp.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.min(X))\n",
    "print(torch.max(X))\n",
    "\n",
    "normalized = (X-torch.min(X))/(torch.max(X)-torch.min(X))\n",
    "print(normalized.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = 'castle.jpg'\n",
    "# Returns a numpy array in BGR color space, not RGB\n",
    "img = cv2.imread(path)\n",
    "\n",
    "# Convert from BGR to RGB color space\n",
    "img = img[..., ::-1]\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).reshape(1, -1, 1, 1)\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).reshape(1, -1, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "# X has shape (1, 3, 224, 224)\n",
    "# Normalize X by subtracting mean and dividing by standard deviation\n",
    "X = (torch.FloatTensor(img[np.newaxis].transpose(\n",
    "    [0, 3, 1, 2])*1) - mean) / std\n",
    "# plt.imshow(X[0].sum(dim=0).detach().numpy(), cmap=cmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = (torch.FloatTensor(img[np.newaxis].transpose(\n",
    "        [0, 3, 1, 2])*1))\n",
    "      \n",
    "print(Y.shape)\n",
    "print(Y[0].sum(dim=0).detach().numpy().shape)\n",
    "plt.imshow(Y[0].sum(dim=0).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "im = transforms.ToPILImage()(X[0])\n",
    "display(im)\n",
    "print(im)\n",
    "print(im.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from myutils import ILSVRC2012_BatchNormalize\n",
    "batch_size = 1\n",
    "shape = (batch_size, 3, 224, 224)\n",
    "norm_fn = ILSVRC2012_BatchNormalize()\n",
    "l = norm_fn(torch.zeros(*shape))\n",
    "h = norm_fn(torch.ones(*shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(low.shape)\n",
    "print(high.shape)\n",
    "print(lrp.model.forward(X).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrp.model.features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo:\n",
    "\n",
    "- Extend mapping to classification layers (model.children or model.modules)\n",
    "- Apply different rules by layer type and index\n",
    "Source: https://github.com/adrhill/ExplainabilityMethods.jl/blob/0a9ca79d75525f99eb59a5d54bc7b0e7486ac614/src/lrp_rules.jl#L84-L88\n",
    "\n",
    "## Automation\n",
    "\n",
    "- Automate modifying the parameters of the model (named_parameters(), see zennit)\n",
    "- Looping through layers of model (children, modules)\n",
    "\n",
    "--- \n",
    "\n",
    "## Hard\n",
    "\n",
    "- Canonize models\n",
    "- Convert MaxPooling2D to AvgPooling2D\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0431170e0a69c05b0d16d782151a0ffea81a7c290afc025c33023a7a723c05b4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
